{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbe3a5c",
   "metadata": {},
   "source": [
    "# Training the CF explainer and getting a CF example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4eda57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janneke/opt/anaconda3/envs/CF-GNNExplainer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# first import the needed packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from gcn import *\n",
    "from gcn_perturbation_matrix import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "from explainer_framework import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4124c8",
   "metadata": {},
   "source": [
    "### Read in the data we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb28c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/syn1.pickle','rb') as pickle_file: \n",
    "    data_syn1 = pickle.load(pickle_file)\n",
    "\n",
    "with open('data/syn4.pickle','rb') as pickle_file:\n",
    "    data_syn4 = pickle.load(pickle_file)\n",
    "    \n",
    "with open('data/syn5.pickle','rb') as pickle_file:\n",
    "    data_syn5 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02df5e",
   "metadata": {},
   "source": [
    "### Put data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0440312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeeze the labels (as it has a singleton dim and then make it a tensor)\n",
    "labels_syn1 = np.squeeze(data_syn1['labels'])\n",
    "labels_syn1 = torch.tensor(labels_syn1)\n",
    "\n",
    "labels_syn4 = np.squeeze(data_syn4['labels'])\n",
    "labels_syn4 = torch.tensor(labels_syn4)\n",
    "\n",
    "labels_syn5 = np.squeeze(data_syn5['labels'])\n",
    "labels_syn5 = torch.tensor(labels_syn5)\n",
    "\n",
    "# same for features, but define the type of data here\n",
    "features_syn1 = np.squeeze(data_syn1['feat'])\n",
    "features_syn1 = torch.tensor(features_syn1, dtype=torch.float)\n",
    "\n",
    "features_syn4 = np.squeeze(data_syn4['feat'])\n",
    "features_syn4 = torch.tensor(features_syn4, dtype=torch.float)\n",
    "\n",
    "features_syn5 = np.squeeze(data_syn5['feat'])\n",
    "features_syn5 = torch.tensor(features_syn5, dtype=torch.float)\n",
    "\n",
    "# adjacency matrix\n",
    "adjacency_matrix_syn1 = torch.tensor(np.squeeze(data_syn1['adj']), dtype=torch.float)\n",
    "adjacency_matrix_syn4 = torch.tensor(np.squeeze(data_syn4['adj']), dtype=torch.float)\n",
    "adjacency_matrix_syn5 = torch.tensor(np.squeeze(data_syn5['adj']), dtype=torch.float)\n",
    "\n",
    "# the indices are already a list --> but have to split the training data in training and validation data first\n",
    "train_indices_full_syn1 = torch.tensor(data_syn1['train_idx'])\n",
    "train_indices_full_syn4 = torch.tensor(data_syn4['train_idx'])\n",
    "train_indices_full_syn5 = torch.tensor(data_syn5['train_idx'])\n",
    "\n",
    "# split in training and validation indices\n",
    "train_indices_syn1, validation_indices_syn1 = torch.utils.data.random_split(train_indices_full_syn1, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_indices_syn4, validation_indices_syn4 = torch.utils.data.random_split(train_indices_full_syn4, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_indices_syn5, validation_indices_syn5 = torch.utils.data.random_split(train_indices_full_syn5, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "test_indices_syn1 = torch.tensor(data_syn1['test_idx'])\n",
    "test_indices_syn4 = torch.tensor(data_syn4['test_idx'])\n",
    "test_indices_syn5 = torch.tensor(data_syn5['test_idx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f0192",
   "metadata": {},
   "source": [
    "### Get the original models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbdd5bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn1 = torch.load('models/syn1model.pt')\n",
    "model_syn4 = torch.load('models/syn4model.pt')\n",
    "model_syn5 = torch.load('models/syn5model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6821b9",
   "metadata": {},
   "source": [
    "### First, get the original predictions for the model we're researching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b854dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of Syn1 data:  0.9571428571428572\n",
      "Test accuracy of Syn4 data:  0.88\n",
      "Test accuracy of Syn5 data:  0.7854251012145749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janneke/PycharmProjects/CF-GNNExplainer/gcn.py:147: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525473998/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  A_hat = torch.sparse_coo_tensor((A_hat.row, A_hat.col), A_hat.data, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "model_syn1.eval()\n",
    "sparse_adj_1 = get_sparse_adjacency_normalized(features_syn1.shape[0], adjacency_matrix_syn1)\n",
    "outputs_syn1 = model_syn1(features_syn1, sparse_adj_1)\n",
    "\n",
    "# print accuracy too (to check that it is the same as in the original)\n",
    "_, predictions_1 = torch.max(outputs_syn1.data, 1)\n",
    "print(\"Test accuracy of Syn1 data: \", accuracy_score(labels_syn1[test_indices_syn1], predictions_1[test_indices_syn1]))\n",
    "\n",
    "model_syn4.eval()\n",
    "sparse_adj_4 = get_sparse_adjacency_normalized(features_syn4.shape[0], adjacency_matrix_syn4)\n",
    "outputs_syn4 = model_syn4(features_syn4, sparse_adj_4)\n",
    "\n",
    "# print accuracy too (to check that it is the same as in the original)\n",
    "_, predictions_4 = torch.max(outputs_syn4.data, 1)\n",
    "print(\"Test accuracy of Syn4 data: \", accuracy_score(labels_syn4[test_indices_syn4], predictions_4[test_indices_syn4]))\n",
    "\n",
    "model_syn5.eval()\n",
    "sparse_adj_5 = get_sparse_adjacency_normalized(features_syn5.shape[0], adjacency_matrix_syn5)\n",
    "outputs_syn5 = model_syn5(features_syn5, sparse_adj_5)\n",
    "\n",
    "# print accuracy too (to check that it is the same as in the original)\n",
    "_, predictions_5 = torch.max(outputs_syn5.data, 1)\n",
    "print(\"Test accuracy of Syn5 data: \", accuracy_score(labels_syn5[test_indices_syn5], predictions_5[test_indices_syn5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92caf2d",
   "metadata": {},
   "source": [
    "### Get the weights and biases for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa872c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_W_syn1 = model_syn1.gcn_layer_1.W.detach()\n",
    "layer1_b_syn1 = model_syn1.gcn_layer_1.b.detach()\n",
    "layer2_W_syn1 = model_syn1.gcn_layer_2.W.detach()\n",
    "layer2_b_syn1 = model_syn1.gcn_layer_2.b.detach()\n",
    "layer3_W_syn1 = model_syn1.gcn_layer_3.W.detach()\n",
    "layer3_b_syn1 = model_syn1.gcn_layer_3.b.detach()\n",
    "lin_weight_syn1 = model_syn1.linear_layer.weight.detach()\n",
    "lin_b_syn1 = model_syn1.linear_layer.bias.detach()\n",
    "\n",
    "layer1_W_syn4 = model_syn4.gcn_layer_1.W.detach()\n",
    "layer1_b_syn4 = model_syn4.gcn_layer_1.b.detach()\n",
    "layer2_W_syn4 = model_syn4.gcn_layer_2.W.detach()\n",
    "layer2_b_syn4 = model_syn4.gcn_layer_2.b.detach()\n",
    "layer3_W_syn4 = model_syn4.gcn_layer_3.W.detach()\n",
    "layer3_b_syn4 = model_syn4.gcn_layer_3.b.detach()\n",
    "lin_weight_syn4 = model_syn4.linear_layer.weight.detach()\n",
    "lin_b_syn4 = model_syn4.linear_layer.bias.detach()\n",
    "\n",
    "layer1_W_syn5 = model_syn5.gcn_layer_1.W.detach()\n",
    "layer1_b_syn5 = model_syn5.gcn_layer_1.b.detach()\n",
    "layer2_W_syn5 = model_syn5.gcn_layer_2.W.detach()\n",
    "layer2_b_syn5 = model_syn5.gcn_layer_2.b.detach()\n",
    "layer3_W_syn5 = model_syn5.gcn_layer_3.W.detach()\n",
    "layer3_b_syn5 = model_syn5.gcn_layer_3.b.detach()\n",
    "lin_weight_syn5 = model_syn5.linear_layer.weight.detach()\n",
    "lin_b_syn5 = model_syn5.linear_layer.bias.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789b2e9",
   "metadata": {},
   "source": [
    "### Set up structure to get CF-examples (new perturbed GCN for every node we get a CF-explanation for!)\n",
    "As subgraphs may have different sizes--> first get subgraph, then we know how big the perturbation matrix should be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebcb671",
   "metadata": {},
   "source": [
    "## Train it and get the CF-examples!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "accef3aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "examples_all = []\n",
    "train_loss_all = []\n",
    "nr_cf = 0\n",
    "\n",
    "for index in test_indices_syn1: # we first do these three to test whether everything works properly!!\n",
    "    # get the old prediction\n",
    "    old_prediction = predictions_1[index.item()]\n",
    "    \n",
    "    # get the subgraph neighbourhood\n",
    "    adjacency_matrix, vertex_mapping, labels_perturbed, features_perturbed = create_subgraph_neighbourhood2(index.item(), 4, labels_syn1, features_syn1, adjacency_matrix_syn1)\n",
    "    \n",
    "    new_index = vertex_mapping[index.item()]\n",
    "    \n",
    "    # test whether it gets the same outcome\n",
    "    sparse_adj_test = get_sparse_adjacency_normalized(features_perturbed.shape[0], adjacency_matrix)\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model_syn1(features_perturbed, sparse_adj_test)\n",
    "\n",
    "    # get accuracy too (to check that it is the same as in the original)\n",
    "    _, predictions_test = torch.max(outputs_test.data, 1)\n",
    "    \n",
    "    # as a small test:\n",
    "    assert predictions_test[new_index].item() == old_prediction, \"wrong prediction\"\n",
    "        \n",
    "    # make a gcn model (to use for perturbation):\n",
    "    model_pert = GCNPerturbed(layer1_W_syn1, layer1_b_syn1, layer2_W_syn1, layer2_b_syn1, layer3_W_syn1, layer3_b_syn1, lin_weight_syn1, lin_b_syn1, adjacency_matrix.shape[0])\n",
    "    \n",
    "    # from the model hyperparams:\n",
    "    alpha = 0.1\n",
    "    optim = torch.optim.SGD(model_pert.parameters(), lr=alpha, nesterov=True, momentum=0.9)\n",
    "    beta = 0.5\n",
    "    k = 500\n",
    "    \n",
    "    # get the new cf example!\n",
    "    examples_for_index, train_loss = get_cf_example(new_index, old_prediction, model_pert, optim, beta, k, adjacency_matrix, labels_perturbed, features_perturbed)\n",
    "    \n",
    "    # append to all examples!!\n",
    "    examples_all.append(examples_for_index)\n",
    "    train_loss_all.append(train_loss)\n",
    "    \n",
    "    if examples_for_index != []:\n",
    "        nr_cf = nr_cf + 1\n",
    "\n",
    "print(nr_cf)\n",
    "print(len(test_indices_syn1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93a65e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.23569774627685547,\n",
       " -0.2413913905620575,\n",
       " -0.24997884035110474,\n",
       " -0.26172947883605957,\n",
       " -0.2771143317222595,\n",
       " -0.2968655228614807,\n",
       " -0.3221174478530884,\n",
       " -0.35453811287879944,\n",
       " -0.3966902792453766,\n",
       " -0.4525355398654938,\n",
       " -0.5284311771392822,\n",
       " -0.6347899436950684,\n",
       " -0.7891736030578613,\n",
       " 0.5,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5,\n",
       " 1.5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f2b4f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_for_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdae259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "4.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "3.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "4.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "4.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "3.0\n",
      "20.0\n",
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "3.0\n",
      "3.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "6.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "7.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "9.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "expl_size = []\n",
    "\n",
    "for i in examples_all:\n",
    "    if i != []:\n",
    "        expl_size.append((sum(sum(i[-1] == 0))/2).item())\n",
    "        print((sum(sum(i[0] == 0))/2).item())\n",
    "\n",
    "mean(expl_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a5132f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1311475409836067"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import mean\n",
    "mean(expl_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d252e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714285714285714"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "122/140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d310b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1,\n",
       "        2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1,\n",
       "        1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3,\n",
       "        1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2,\n",
       "        3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2,\n",
       "        2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1,\n",
       "        2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1,\n",
       "        1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3,\n",
       "        1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2,\n",
       "        3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2,\n",
       "        2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1,\n",
       "        2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1,\n",
       "        1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3,\n",
       "        1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2,\n",
       "        3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2,\n",
       "        2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1,\n",
       "        2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1, 1, 2, 2, 3, 1,\n",
       "        1, 2, 2, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_syn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85a8d1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_indices_syn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m         nr_cf_5 \u001b[38;5;241m=\u001b[39m nr_cf_5 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(nr_cf_5)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtest_indices_syn\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_indices_syn' is not defined"
     ]
    }
   ],
   "source": [
    "examples_all = []\n",
    "train_loss_all = []\n",
    "nr_cf_5 = 0\n",
    "\n",
    "for index in test_indices_syn5: # we first do these three to test whether everything works properly!!\n",
    "    # get the old prediction\n",
    "    old_prediction = predictions_5[index.item()]\n",
    "    \n",
    "    # get the subgraph neighbourhood\n",
    "    adjacency_matrix, vertex_mapping, labels_perturbed, features_perturbed = create_subgraph_neighbourhood2(index.item(), 4, labels_syn5, features_syn5, adjacency_matrix_syn5)\n",
    "    \n",
    "    new_index = vertex_mapping[index.item()]\n",
    "    \n",
    "    # test whether it gets the same outcome\n",
    "    sparse_adj_test = get_sparse_adjacency_normalized(features_perturbed.shape[0], adjacency_matrix)\n",
    "    with torch.no_grad():\n",
    "        outputs_test = model_syn5(features_perturbed, sparse_adj_test)\n",
    "\n",
    "    # get accuracy too (to check that it is the same as in the original)\n",
    "    _, predictions_test = torch.max(outputs_test.data, 1)\n",
    "    \n",
    "    # as a small test:\n",
    "    assert predictions_test[new_index].item() == old_prediction, \"wrong prediction\"\n",
    "        \n",
    "    # make a gcn model (to use for perturbation):\n",
    "    model_pert = GCNPerturbed(layer1_W_syn5, layer1_b_syn5, layer2_W_syn5, layer2_b_syn5, layer3_W_syn5, layer3_b_syn5, lin_weight_syn5, lin_b_syn5, adjacency_matrix.shape[0])\n",
    "    \n",
    "    # from the model hyperparams:\n",
    "    alpha = 0.1\n",
    "    optim = torch.optim.SGD(model_pert.parameters(), lr=alpha)\n",
    "    beta = 0.5\n",
    "    k = 500\n",
    "    \n",
    "    # get the new cf example!\n",
    "    examples_for_index, train_loss = get_cf_example(new_index, old_prediction, model_pert, optim, beta, k, adjacency_matrix, labels_perturbed, features_perturbed)\n",
    "    \n",
    "    # append to all examples!!\n",
    "    examples_all.append(examples_for_index)\n",
    "    train_loss_all.append(train_loss)\n",
    "    \n",
    "    if examples_for_index != []:\n",
    "        nr_cf_5 = nr_cf_5 + 1\n",
    "\n",
    "print(nr_cf_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb13b5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "3.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.52"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl_size = []\n",
    "\n",
    "for i in examples_all:\n",
    "    if i != []:\n",
    "        expl_size.append((sum(sum(i[-1] == 0))/2).item())\n",
    "        print((sum(sum(i[0] == 0))/2).item())\n",
    "\n",
    "mean(expl_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8aa1543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.708502024291498\n"
     ]
    }
   ],
   "source": [
    "print(nr_cf_5/ len(test_indices_syn5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbb76b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CFGNNExplainerKernel",
   "language": "python",
   "name": "cfgnnexplainerkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
