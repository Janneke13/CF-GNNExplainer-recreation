{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13ddf15",
   "metadata": {},
   "source": [
    "# Baselines and their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c62f80e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janneke/opt/anaconda3/envs/CF-GNNExplainer/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from gcn import *\n",
    "from gcn_perturbation_matrix import *\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from calculate_metrics import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76c754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/syn1.pickle','rb') as pickle_file: \n",
    "    data_syn1 = pickle.load(pickle_file)\n",
    "\n",
    "with open('data/syn4.pickle','rb') as pickle_file:\n",
    "    data_syn4 = pickle.load(pickle_file)\n",
    "    \n",
    "with open('data/syn5.pickle','rb') as pickle_file:\n",
    "    data_syn5 = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba4ec4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# squeeze the labels (as it has a singleton dim and then make it a tensor)\n",
    "labels_syn1 = np.squeeze(data_syn1['labels'])\n",
    "labels_syn1 = torch.tensor(labels_syn1)\n",
    "\n",
    "labels_syn4 = np.squeeze(data_syn4['labels'])\n",
    "labels_syn4 = torch.tensor(labels_syn4)\n",
    "\n",
    "labels_syn5 = np.squeeze(data_syn5['labels'])\n",
    "labels_syn5 = torch.tensor(labels_syn5)\n",
    "\n",
    "# same for features, but define the type of data here\n",
    "features_syn1 = np.squeeze(data_syn1['feat'])\n",
    "features_syn1 = torch.tensor(features_syn1, dtype=torch.float)\n",
    "\n",
    "features_syn4 = np.squeeze(data_syn4['feat'])\n",
    "features_syn4 = torch.tensor(features_syn4, dtype=torch.float)\n",
    "\n",
    "features_syn5 = np.squeeze(data_syn5['feat'])\n",
    "features_syn5 = torch.tensor(features_syn5, dtype=torch.float)\n",
    "\n",
    "# adjacency matrix\n",
    "adjacency_matrix_syn1 = torch.tensor(np.squeeze(data_syn1['adj']), dtype=torch.float)\n",
    "adjacency_matrix_syn4 = torch.tensor(np.squeeze(data_syn4['adj']), dtype=torch.float)\n",
    "adjacency_matrix_syn5 = torch.tensor(np.squeeze(data_syn5['adj']), dtype=torch.float)\n",
    "\n",
    "# the indices are already a list --> but have to split the training data in training and validation data first\n",
    "train_indices_full_syn1 = torch.tensor(data_syn1['train_idx'])\n",
    "train_indices_full_syn4 = torch.tensor(data_syn4['train_idx'])\n",
    "train_indices_full_syn5 = torch.tensor(data_syn5['train_idx'])\n",
    "\n",
    "# split in training and validation indices\n",
    "train_indices_syn1, validation_indices_syn1 = torch.utils.data.random_split(train_indices_full_syn1, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_indices_syn4, validation_indices_syn4 = torch.utils.data.random_split(train_indices_full_syn4, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_indices_syn5, validation_indices_syn5 = torch.utils.data.random_split(train_indices_full_syn5, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "test_indices_syn1 = torch.tensor(data_syn1['test_idx'])\n",
    "test_indices_syn4 = torch.tensor(data_syn4['test_idx'])\n",
    "test_indices_syn5 = torch.tensor(data_syn5['test_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0a55e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_syn1 = torch.load('models/syn1model.pt')\n",
    "model_syn4 = torch.load('models/syn4model.pt')\n",
    "model_syn5 = torch.load('models/syn5model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e171d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of Syn1 data:  0.9928571428571429\n",
      "Test accuracy of Syn4 data:  0.9142857142857143\n",
      "Test accuracy of Syn5 data:  0.8623481781376519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janneke/Documents/GitHub/CF-GNNExplainer-recreation/gcn.py:159: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525473998/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  A_hat = torch.sparse_coo_tensor((A_hat.row, A_hat.col), A_hat.data, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "model_syn1.eval()\n",
    "sparse_adj_1 = get_sparse_adjacency_normalized(features_syn1.shape[0], adjacency_matrix_syn1)\n",
    "outputs_syn1 = model_syn1(features_syn1, sparse_adj_1)\n",
    "\n",
    "# print accuracy too (to check that it is the same as in the original)\n",
    "_, predictions_1 = torch.max(outputs_syn1.data, 1)\n",
    "print(\"Test accuracy of Syn1 data: \", accuracy_score(labels_syn1[test_indices_syn1], predictions_1[test_indices_syn1]))\n",
    "\n",
    "model_syn4.eval()\n",
    "sparse_adj_4 = get_sparse_adjacency_normalized(features_syn4.shape[0], adjacency_matrix_syn4)\n",
    "outputs_syn4 = model_syn4(features_syn4, sparse_adj_4)\n",
    "\n",
    "# print accuracy too (to check that it is the same as in the original)\n",
    "_, predictions_4 = torch.max(outputs_syn4.data, 1)\n",
    "print(\"Test accuracy of Syn4 data: \", accuracy_score(labels_syn4[test_indices_syn4], predictions_4[test_indices_syn4]))\n",
    "\n",
    "model_syn5.eval()\n",
    "sparse_adj_5 = get_sparse_adjacency_normalized(features_syn5.shape[0], adjacency_matrix_syn5)\n",
    "outputs_syn5 = model_syn5(features_syn5, sparse_adj_5)\n",
    "\n",
    "# print accuracy too (to check that it is the same as in the original)\n",
    "_, predictions_5 = torch.max(outputs_syn5.data, 1)\n",
    "print(\"Test accuracy of Syn5 data: \", accuracy_score(labels_syn5[test_indices_syn5], predictions_5[test_indices_syn5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec04cac",
   "metadata": {},
   "source": [
    "## Random baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e1665",
   "metadata": {},
   "source": [
    "Steps of the random baseline (according to the paper):\n",
    "1. Randomly initialize P_hat in [-1,1]\n",
    "2. Apply sigmoid and threshold\n",
    "3. Do this for K times, keep minimal CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4543f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline(subgraph, k, model, old_prediction, feat, index):\n",
    "    min_deleted = float('inf')\n",
    "    perturbation_matrix = []\n",
    "    \n",
    "    for number in range(k):\n",
    "        size_vector = int(subgraph.shape[0] * (subgraph.shape[0] + 1) / 2)\n",
    "        \n",
    "        p_hat = torch.FloatTensor(size_vector).uniform_(-1,1)\n",
    "        pert = sigmoid(p_hat)\n",
    "        pert = (pert > 0.5).float()\n",
    "\n",
    "        # populate the matrix symmetrically\n",
    "        index_row, index_col = torch.triu_indices(subgraph.shape[0], subgraph.shape[0])\n",
    "\n",
    "        P = torch.zeros(subgraph.shape[0], subgraph.shape[0])\n",
    "        P[index_row, index_col] = pert\n",
    "        P.T[index_row, index_col] = pert\n",
    "        \n",
    "        # get the new matrix and how many edges were deleted in total\n",
    "        new_matrix = torch.mul(subgraph, P)\n",
    "        nr_deleted = torch.sum((new_matrix != subgraph).float()).data / 2\n",
    "        \n",
    "        # find new prediction:\n",
    "        sparse_adj = get_sparse_adjacency_normalized(feat.shape[0], new_matrix)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(feat, sparse_adj)\n",
    "            \n",
    "        _, predictions_test = torch.max(outputs.data, 1)\n",
    "        new_prediction = predictions_test[index].item()\n",
    "        \n",
    "        if nr_deleted < min_deleted and new_prediction != old_prediction:\n",
    "            perturbation_matrix.append(P)\n",
    "    \n",
    "    return perturbation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5fe7e6",
   "metadata": {},
   "source": [
    "## 1-Hop baseline and remove 1-Hop baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abbb73",
   "metadata": {},
   "source": [
    "Steps of the 1-Hop baseline (according to the paper):\n",
    "1. Get ego-graph of node\n",
    "2. Remove every edge not in the ego graph\n",
    "\n",
    "Steps of the Remove 1-Hop baseline (according to the paper):\n",
    "1. Get ego-graph of node\n",
    "2. Remove every edge in the ego graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02458c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hop_baselines(subgraph, model, old_prediction, feat, node):\n",
    "    # return both the perturbation matrix for the one-hop and remove one-hop baselines IF it is a explanation\n",
    "    # in same format as before (list of tensors)\n",
    "    perturbation_matrix_one_hop = [] \n",
    "    perturbation_matrix_remove_one_hop = [] \n",
    "    \n",
    "    # collect all NODES that are one hop away - similar to how the neighbourhood was found before\n",
    "    vertices = set([node])\n",
    "    \n",
    "    for vertex_1 in list(vertices):  # loop over all vertices already in the set --> this is the column we're in\n",
    "        for vertex_2 in range(subgraph.shape[0]):  # loop over every incoming connection of a certain vertex - this is the row\n",
    "            if subgraph[vertex_2, vertex_1] != 0:\n",
    "                vertices.add(vertex_2)  # add the vertex if there is a connection\n",
    "    \n",
    "    vertex_list = list(vertices)\n",
    "    vertex_list_not = [i for i in list(range(subgraph.shape[0])) if i not in vertex_list ]\n",
    "    \n",
    "    vertex_list = torch.tensor(vertex_list)\n",
    "    vertex_list_not = torch.tensor(vertex_list_not)\n",
    "    \n",
    "    # for one hop: keep all these nodes --> zero out every other row/column    \n",
    "    one_hop_pert = torch.ones(subgraph.shape[0], subgraph.shape[0])\n",
    "    one_hop_pert[vertex_list_not, :] = 0\n",
    "    one_hop_pert[:, vertex_list_not] = 0\n",
    "\n",
    "    # for remove one hop: zero out these rows/columns, keep the rest\n",
    "    one_hop_pert_rm = torch.ones(subgraph.shape[0], subgraph.shape[0])\n",
    "    one_hop_pert_rm[vertex_list, :] = 0\n",
    "    one_hop_pert_rm[:, vertex_list] = 0\n",
    "    \n",
    "    # perturb:\n",
    "    one_hop_subgraph = torch.mul(subgraph, one_hop_pert)\n",
    "    one_hop_subgraph_rm = torch.mul(subgraph, one_hop_pert_rm)\n",
    "    \n",
    "    # check whether it is counterfactual - one hop:\n",
    "    sparse_adj_one_hop = get_sparse_adjacency_normalized(feat.shape[0], one_hop_subgraph)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs_one_hop = model(feat, sparse_adj_one_hop)\n",
    "            \n",
    "    _, predictions_test = torch.max(outputs_one_hop.data, 1)\n",
    "    new_prediction_one_hop = predictions_test[node].item()\n",
    "    \n",
    "    # check whether it is counterfactual - one hop removed:\n",
    "    sparse_adj_one_hop_rm = get_sparse_adjacency_normalized(feat.shape[0], one_hop_subgraph_rm)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        outputs_one_hop_rm = model(feat, sparse_adj_one_hop_rm)\n",
    "            \n",
    "    _, predictions_test_rm = torch.max(outputs_one_hop_rm.data, 1)\n",
    "    new_prediction_one_hop_rm = predictions_test_rm[node].item()\n",
    "        \n",
    "    # add to list if it is a counterexample\n",
    "    \n",
    "    if new_prediction_one_hop != old_prediction:\n",
    "        perturbation_matrix_one_hop.append(one_hop_pert)\n",
    "    \n",
    "    if new_prediction_one_hop_rm != old_prediction:\n",
    "        perturbation_matrix_remove_one_hop.append(one_hop_pert_rm)\n",
    "    \n",
    "    return perturbation_matrix_one_hop, perturbation_matrix_remove_one_hop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ee0e1",
   "metadata": {},
   "source": [
    "## Run all the baselines:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e26b49",
   "metadata": {},
   "source": [
    "### Syn 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60aeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "examples_all_random_syn1 = []\n",
    "examples_all_one_hop_syn1 = []\n",
    "examples_all_one_hop_rm_syn1 = []\n",
    "\n",
    "adjacency_neigh_syn1 = []\n",
    "mapping_syn1 = []\n",
    "\n",
    "nr_cf_syn1_rand = 0\n",
    "nr_cf_syn1_one_hop = 0\n",
    "nr_cf_syn1_one_hop_rm = 0\n",
    "\n",
    "k = 500\n",
    "\n",
    "for index in test_indices_syn1: \n",
    "    old_prediction = predictions_1[index.item()]\n",
    "    \n",
    "    # get the subgraph neighbourhood\n",
    "    adjacency_matrix, vertex_mapping, labels_perturbed, features_perturbed = create_subgraph_neighbourhood2(index.item(), 4, labels_syn1, features_syn1, adjacency_matrix_syn1)\n",
    "    \n",
    "    new_index = vertex_mapping[index.item()]\n",
    "    \n",
    "    # for random\n",
    "    perturbation_random = random_baseline(adjacency_matrix, k, model_syn1, old_prediction, features_perturbed, new_index)\n",
    "    \n",
    "    # for one hop and remove one hop:\n",
    "    one_hop_pert, remove_one_hop_pert =  one_hop_baselines(adjacency_matrix, model_syn1, old_prediction, features_perturbed, new_index)\n",
    "    \n",
    "    # put all the ones here:\n",
    "    examples_all_random_syn1.append(perturbation_random)\n",
    "    # add 1-hop\n",
    "    examples_all_one_hop_syn1.append(one_hop_pert)\n",
    "    # add remove 1-hop\n",
    "    examples_all_one_hop_rm_syn1.append(remove_one_hop_pert)\n",
    "    # add gnnexplainer\n",
    "    \n",
    "    \n",
    "    adjacency_neigh_syn1.append(adjacency_matrix)\n",
    "    mapping_syn1.append(vertex_mapping)\n",
    "    \n",
    "    if perturbation_random != []:\n",
    "        nr_cf_syn1_rand+= 1\n",
    "    if one_hop_pert != []:\n",
    "        nr_cf_syn1_one_hop += 1\n",
    "    if remove_one_hop_pert != []:  \n",
    "        nr_cf_syn1_one_hop_rm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e60ae05",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m sparsity_mean_random, sparsity_std_random \u001b[38;5;241m=\u001b[39m sparsity(adjacency_neigh_syn1, examples_all_random_syn1)\n\u001b[1;32m      3\u001b[0m fidelity_syn1_random \u001b[38;5;241m=\u001b[39m fidelity(\u001b[38;5;28mlen\u001b[39m(test_indices_syn1), nr_cf_syn1_rand)\n\u001b[0;32m----> 4\u001b[0m accuracy_syn1_random, accuracy_syn1_std_random \u001b[38;5;241m=\u001b[39m accuracy_explanation(test_indices_syn1, predictions_1, adjacency_neigh_syn1, examples_all_random_syn1, mapping_syn1)\n\u001b[1;32m      5\u001b[0m explanation_size_mean_random, explanation_size_std_random, explanation_size_list_syn1_random \u001b[38;5;241m=\u001b[39m explanation_size(examples_all_random_syn1, adjacency_neigh_syn1)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# for one hop baseline\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# for random baseline\n",
    "sparsity_mean_random, sparsity_std_random = sparsity(adjacency_neigh_syn1, examples_all_random_syn1)\n",
    "fidelity_syn1_random = fidelity(len(test_indices_syn1), nr_cf_syn1_rand)\n",
    "accuracy_syn1_random, accuracy_syn1_std_random = accuracy_explanation(test_indices_syn1, predictions_1, adjacency_neigh_syn1, examples_all_random_syn1, mapping_syn1)\n",
    "explanation_size_mean_random, explanation_size_std_random, explanation_size_list_syn1_random = explanation_size(examples_all_random_syn1, adjacency_neigh_syn1)\n",
    "\n",
    "# for one hop baseline\n",
    "sparsity_mean_oh, sparsity_std_oh = sparsity(adjacency_neigh_syn1, examples_all_one_hop_syn1)\n",
    "fidelity_syn1_oh = fidelity(len(test_indices_syn1), nr_cf_syn1_one_hop)\n",
    "accuracy_syn1_oh, accuracy_syn1_std_oh = accuracy_explanation(test_indices_syn1, predictions_1, adjacency_neigh_syn1, examples_all_one_hop_syn1, mapping_syn1)\n",
    "explanation_size_mean_oh, explanation_size_std_oh, explanation_size_list_syn1_oh = explanation_size(examples_all_one_hop_syn1, adjacency_neigh_syn1)\n",
    "\n",
    "# for one hop rm baseline\n",
    "sparsity_mean_oh_rm, sparsity_std_oh_rm = sparsity(adjacency_neigh_syn1, examples_all_one_hop_rm_syn1)\n",
    "fidelity_syn1_oh_rm = fidelity(len(test_indices_syn1), nr_cf_syn1_one_hop_rm)\n",
    "accuracy_syn1_oh_rm, accuracy_syn1_std_oh_rm = accuracy_explanation(test_indices_syn1, predictions_1, adjacency_neigh_syn1, examples_all_one_hop_rm_syn1, mapping_syn1)\n",
    "explanation_size_mean_oh_rm, explanation_size_std_oh_rm, explanation_size_list_syn1_oh_rm = explanation_size(examples_all_one_hop_rm_syn1, adjacency_neigh_syn1)\n",
    "\n",
    "\n",
    "print(\"Syn1 Random Baseline, fidelity:\", fidelity_syn1_random)\n",
    "print(\"Syn1 Random Baseline, mean sparsity:\", sparsity_mean_random)\n",
    "print(\"Syn1 Random Baseline, mean accuracy:\", accuracy_syn1_random)\n",
    "print(\"Syn1 Random Baseline, mean explanation size:\", explanation_size_mean_random)\n",
    "print()\n",
    "print(\"Syn1 Random Baseline, std sparsity:\", sparsity_std_random)\n",
    "print(\"Syn1 Random Baseline, std accuracy:\", accuracy_syn1_std_random)\n",
    "print(\"Syn1 Random Baseline, std explanation size:\", explanation_size_std_random)\n",
    "print(\"------\")\n",
    "print(\"Syn1 One-Hop Baseline, fidelity:\", fidelity_syn1_oh)\n",
    "print(\"Syn1 One-Hop Baseline, mean sparsity:\", sparsity_mean_oh)\n",
    "print(\"Syn1 One-Hop Baseline, mean accuracy:\", accuracy_syn1_oh)\n",
    "print(\"Syn1 One-Hop Baseline, mean explanation size:\", explanation_size_mean_oh)\n",
    "print()\n",
    "print(\"Syn1 One-Hop Baseline, std sparsity:\", sparsity_std_oh)\n",
    "print(\"Syn1 One-Hop Baseline, std accuracy:\", accuracy_syn1_std_oh)\n",
    "print(\"Syn1 One-Hop Baseline, std explanation size:\", explanation_size_std_oh)\n",
    "print(\"------\")\n",
    "print(\"Syn1 RM One-Hop Baseline, fidelity:\", fidelity_syn1_oh_rm)\n",
    "print(\"Syn1 RM One-Hop Baseline, mean sparsity:\", sparsity_mean_oh_rm)\n",
    "print(\"Syn1 RM One-Hop Baseline, mean accuracy:\", accuracy_syn1_oh_rm)\n",
    "print(\"Syn1 RM One-Hop Baseline, mean explanation size:\", explanation_size_mean_oh_rm)\n",
    "print()\n",
    "print(\"Syn1 RM One-Hop Baseline, std sparsity:\", sparsity_std_oh_rm)\n",
    "print(\"Syn1 RM One-Hop Baseline, std accuracy:\", accuracy_syn1_std_oh_rm)\n",
    "print(\"Syn1 RM One-Hop Baseline, std explanation size:\", explanation_size_std_oh_rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb5508",
   "metadata": {},
   "source": [
    "### Syn 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "examples_all_random_syn4 = []\n",
    "examples_all_one_hop_syn4 = []\n",
    "examples_all_one_hop_rm_syn4 = []\n",
    "\n",
    "adjacency_neigh_syn4 = []\n",
    "mapping_syn4 = []\n",
    "\n",
    "nr_cf_syn4_rand = 0\n",
    "nr_cf_syn4_one_hop = 0\n",
    "nr_cf_syn4_one_hop_rm = 0\n",
    "\n",
    "k = 500\n",
    "\n",
    "for index in test_indices_syn4: \n",
    "    old_prediction = predictions_4[index.item()]\n",
    "    \n",
    "    # get the subgraph neighbourhood\n",
    "    adjacency_matrix, vertex_mapping, labels_perturbed, features_perturbed = create_subgraph_neighbourhood2(index.item(), 4, labels_syn4, features_syn4, adjacency_matrix_syn4)\n",
    "    \n",
    "    new_index = vertex_mapping[index.item()]\n",
    "    \n",
    "    # for random\n",
    "    perturbation_random = random_baseline(adjacency_matrix, k, model_syn4, old_prediction, features_perturbed, new_index)\n",
    "    \n",
    "    # for one hop and remove one hop:\n",
    "    one_hop_pert, remove_one_hop_pert =  one_hop_baselines(adjacency_matrix, model_syn4, old_prediction, features_perturbed, new_index)\n",
    "    \n",
    "    # put all the ones here:\n",
    "    examples_all_random_syn4.append(perturbation_random)\n",
    "    # add 1-hop\n",
    "    examples_all_one_hop_syn4.append(one_hop_pert)\n",
    "    # add remove 1-hop\n",
    "    examples_all_one_hop_rm_syn4.append(remove_one_hop_pert)\n",
    "    # add gnnexplainer\n",
    "    \n",
    "    \n",
    "    adjacency_neigh_syn4.append(adjacency_matrix)\n",
    "    mapping_syn4.append(vertex_mapping)\n",
    "    \n",
    "    if perturbation_random != []:\n",
    "        nr_cf_syn4_rand+= 1\n",
    "    if one_hop_pert != []:\n",
    "        nr_cf_syn4_one_hop += 1\n",
    "    if remove_one_hop_pert != []:  \n",
    "        nr_cf_syn4_one_hop_rm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa67486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random baseline\n",
    "sparsity_mean_random, sparsity_std_random = sparsity(adjacency_neigh_syn4, examples_all_random_syn4)\n",
    "fidelity_syn4_random = fidelity(len(test_indices_syn4), nr_cf_syn4_rand)\n",
    "accuracy_syn4_random, accuracy_syn4_std_random = accuracy_explanation(test_indices_syn4, predictions_4, adjacency_neigh_syn4, examples_all_random_syn4, mapping_syn4)\n",
    "explanation_size_mean_random, explanation_size_std_random, explanation_size_list_syn4_random = explanation_size(examples_all_random_syn4, adjacency_neigh_syn4)\n",
    "\n",
    "# for one hop baseline\n",
    "sparsity_mean_oh, sparsity_std_oh = sparsity(adjacency_neigh_syn4, examples_all_one_hop_syn4)\n",
    "fidelity_syn4_oh = fidelity(len(test_indices_syn4), nr_cf_syn4_one_hop)\n",
    "accuracy_syn4_oh, accuracy_syn4_std_oh = accuracy_explanation(test_indices_syn4, predictions_4, adjacency_neigh_syn4, examples_all_one_hop_syn4, mapping_syn4)\n",
    "explanation_size_mean_oh, explanation_size_std_oh, explanation_size_list_syn4_oh = explanation_size(examples_all_one_hop_syn4, adjacency_neigh_syn4)\n",
    "\n",
    "# for one hop rm baseline\n",
    "sparsity_mean_oh_rm, sparsity_std_oh_rm = sparsity(adjacency_neigh_syn4, examples_all_one_hop_rm_syn4)\n",
    "fidelity_syn4_oh_rm = fidelity(len(test_indices_syn4), nr_cf_syn4_one_hop_rm)\n",
    "accuracy_syn4_oh_rm, accuracy_syn4_std_oh_rm = accuracy_explanation(test_indices_syn4, predictions_4, adjacency_neigh_syn4, examples_all_one_hop_rm_syn4, mapping_syn4)\n",
    "explanation_size_mean_oh_rm, explanation_size_std_oh_rm, explanation_size_list_syn4_oh_rm = explanation_size(examples_all_one_hop_rm_syn4, adjacency_neigh_syn4)\n",
    "\n",
    "\n",
    "print(\"Syn4 Random Baseline, fidelity:\", fidelity_syn4_random)\n",
    "print(\"Syn4 Random Baseline, mean sparsity:\", sparsity_mean_random)\n",
    "print(\"Syn4 Random Baseline, mean accuracy:\", accuracy_syn4_random)\n",
    "print(\"Syn4 Random Baseline, mean explanation size:\", explanation_size_mean_random)\n",
    "print()\n",
    "print(\"Syn4 Random Baseline, std sparsity:\", sparsity_std_random)\n",
    "print(\"Syn4 Random Baseline, std accuracy:\", accuracy_syn4_std_random)\n",
    "print(\"Syn4 Random Baseline, std explanation size:\", explanation_size_std_random)\n",
    "print(\"------\")\n",
    "print(\"Syn4 One-Hop Baseline, fidelity:\", fidelity_syn4_oh)\n",
    "print(\"Syn4 One-Hop Baseline, mean sparsity:\", sparsity_mean_oh)\n",
    "print(\"Syn4 One-Hop Baseline, mean accuracy:\", accuracy_syn4_oh)\n",
    "print(\"Syn4 One-Hop Baseline, mean explanation size:\", explanation_size_mean_oh)\n",
    "print()\n",
    "print(\"Syn4 One-Hop Baseline, std sparsity:\", sparsity_std_oh)\n",
    "print(\"Syn4 One-Hop Baseline, std accuracy:\", accuracy_syn4_std_oh)\n",
    "print(\"Syn4 One-Hop Baseline, std explanation size:\", explanation_size_std_oh)\n",
    "print(\"------\")\n",
    "print(\"Syn4 RM One-Hop Baseline, fidelity:\", fidelity_syn4_oh_rm)\n",
    "print(\"Syn4 RM One-Hop Baseline, mean sparsity:\", sparsity_mean_oh_rm)\n",
    "print(\"Syn4 RM One-Hop Baseline, mean accuracy:\", accuracy_syn4_oh_rm)\n",
    "print(\"Syn4 RM One-Hop Baseline, mean explanation size:\", explanation_size_mean_oh_rm)\n",
    "print()\n",
    "print(\"Syn4 RM One-Hop Baseline, std sparsity:\", sparsity_std_oh_rm)\n",
    "print(\"Syn4 RM One-Hop Baseline, std accuracy:\", accuracy_syn4_std_oh_rm)\n",
    "print(\"Syn4 RM One-Hop Baseline, std explanation size:\", explanation_size_std_oh_rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb8bfae",
   "metadata": {},
   "source": [
    "### Syn5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "examples_all_random_syn5 = []\n",
    "examples_all_one_hop_syn5 = []\n",
    "examples_all_one_hop_rm_syn5 = []\n",
    "\n",
    "adjacency_neigh_syn5 = []\n",
    "mapping_syn5 = []\n",
    "\n",
    "nr_cf_syn5_rand = 0\n",
    "nr_cf_syn5_one_hop = 0\n",
    "nr_cf_syn5_one_hop_rm = 0\n",
    "\n",
    "k = 500\n",
    "\n",
    "for index in test_indices_syn5: \n",
    "    old_prediction = predictions_5[index.item()]\n",
    "    \n",
    "    # get the subgraph neighbourhood\n",
    "    adjacency_matrix, vertex_mapping, labels_perturbed, features_perturbed = create_subgraph_neighbourhood2(index.item(), 4, labels_syn5, features_syn5, adjacency_matrix_syn5)\n",
    "    \n",
    "    new_index = vertex_mapping[index.item()]\n",
    "    \n",
    "    # for random\n",
    "    perturbation_random = random_baseline(adjacency_matrix, k, model_syn5, old_prediction, features_perturbed, new_index)\n",
    "    \n",
    "    # for one hop and remove one hop:\n",
    "    one_hop_pert, remove_one_hop_pert =  one_hop_baselines(adjacency_matrix, model_syn5, old_prediction, features_perturbed, new_index)\n",
    "    \n",
    "    # put all the ones here:\n",
    "    examples_all_random_syn5.append(perturbation_random)\n",
    "    # add 1-hop\n",
    "    examples_all_one_hop_syn5.append(one_hop_pert)\n",
    "    # add remove 1-hop\n",
    "    examples_all_one_hop_rm_syn5.append(remove_one_hop_pert)\n",
    "    # add gnnexplainer\n",
    "    \n",
    "    \n",
    "    adjacency_neigh_syn5.append(adjacency_matrix)\n",
    "    mapping_syn5.append(vertex_mapping)\n",
    "    \n",
    "    if perturbation_random != []:\n",
    "        nr_cf_syn5_rand+= 1\n",
    "    if one_hop_pert != []:\n",
    "        nr_cf_syn5_one_hop += 1\n",
    "    if remove_one_hop_pert != []:  \n",
    "        nr_cf_syn5_one_hop_rm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25eb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for random baseline\n",
    "sparsity_mean_random, sparsity_std_random = sparsity(adjacency_neigh_syn5, examples_all_random_syn5)\n",
    "fidelity_syn5_random = fidelity(len(test_indices_syn5), nr_cf_syn5_rand)\n",
    "accuracy_syn5_random, accuracy_syn5_std_random = accuracy_explanation(test_indices_syn5, predictions_5, adjacency_neigh_syn5, examples_all_random_syn5, mapping_syn5)\n",
    "explanation_size_mean_random, explanation_size_std_random, explanation_size_list_syn5_random = explanation_size(examples_all_random_syn5, adjacency_neigh_syn5)\n",
    "\n",
    "# for one hop baseline\n",
    "sparsity_mean_oh, sparsity_std_oh = sparsity(adjacency_neigh_syn5, examples_all_one_hop_syn5)\n",
    "fidelity_syn5_oh = fidelity(len(test_indices_syn5), nr_cf_syn5_one_hop)\n",
    "accuracy_syn5_oh, accuracy_syn5_std_oh = accuracy_explanation(test_indices_syn5, predictions_5, adjacency_neigh_syn5, examples_all_one_hop_syn5, mapping_syn5)\n",
    "explanation_size_mean_oh, explanation_size_std_oh, explanation_size_list_syn5_oh = explanation_size(examples_all_one_hop_syn5, adjacency_neigh_syn5)\n",
    "\n",
    "# for one hop rm baseline\n",
    "sparsity_mean_oh_rm, sparsity_std_oh_rm = sparsity(adjacency_neigh_syn5, examples_all_one_hop_rm_syn5)\n",
    "fidelity_syn5_oh_rm = fidelity(len(test_indices_syn5), nr_cf_syn5_one_hop_rm)\n",
    "accuracy_syn5_oh_rm, accuracy_syn5_std_oh_rm = accuracy_explanation(test_indices_syn5, predictions_5, adjacency_neigh_syn5, examples_all_one_hop_rm_syn5, mapping_syn5)\n",
    "explanation_size_mean_oh_rm, explanation_size_std_oh_rm, explanation_size_list_syn5_oh_rm = explanation_size(examples_all_one_hop_rm_syn5, adjacency_neigh_syn5)\n",
    "\n",
    "\n",
    "print(\"Syn5 Random Baseline, fidelity:\", fidelity_syn5_random)\n",
    "print(\"Syn5 Random Baseline, mean sparsity:\", sparsity_mean_random)\n",
    "print(\"Syn5 Random Baseline, mean accuracy:\", accuracy_syn5_random)\n",
    "print(\"Syn5 Random Baseline, mean explanation size:\", explanation_size_mean_random)\n",
    "print()\n",
    "print(\"Syn5 Random Baseline, std sparsity:\", sparsity_std_random)\n",
    "print(\"Syn5 Random Baseline, std accuracy:\", accuracy_syn5_std_random)\n",
    "print(\"Syn5 Random Baseline, std explanation size:\", explanation_size_std_random)\n",
    "print(\"------\")\n",
    "print(\"Syn5 One-Hop Baseline, fidelity:\", fidelity_syn5_oh)\n",
    "print(\"Syn5 One-Hop Baseline, mean sparsity:\", sparsity_mean_oh)\n",
    "print(\"Syn5 One-Hop Baseline, mean accuracy:\", accuracy_syn5_oh)\n",
    "print(\"Syn5 One-Hop Baseline, mean explanation size:\", explanation_size_mean_oh)\n",
    "print()\n",
    "print(\"Syn5 One-Hop Baseline, std sparsity:\", sparsity_std_oh)\n",
    "print(\"Syn5 One-Hop Baseline, std accuracy:\", accuracy_syn5_std_oh)\n",
    "print(\"Syn5 One-Hop Baseline, std explanation size:\", explanation_size_std_oh)\n",
    "print(\"------\")\n",
    "print(\"Syn5 RM One-Hop Baseline, fidelity:\", fidelity_syn5_oh_rm)\n",
    "print(\"Syn5 RM One-Hop Baseline, mean sparsity:\", sparsity_mean_oh_rm)\n",
    "print(\"Syn5 RM One-Hop Baseline, mean accuracy:\", accuracy_syn5_oh_rm)\n",
    "print(\"Syn5 RM One-Hop Baseline, mean explanation size:\", explanation_size_mean_oh_rm)\n",
    "print()\n",
    "print(\"Syn5 RM One-Hop Baseline, std sparsity:\", sparsity_std_oh_rm)\n",
    "print(\"Syn5 RM One-Hop Baseline, std accuracy:\", accuracy_syn5_std_oh_rm)\n",
    "print(\"Syn5 RM One-Hop Baseline, std explanation size:\", explanation_size_std_oh_rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4ed88b",
   "metadata": {},
   "source": [
    "## Draw graphs for baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb71503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b07531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn1_random, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn1_random_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b9f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn1_oh, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn1_onehop_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ba8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn1_oh_rm, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn1_rm_onehop_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835c278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn4_random, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn4_random_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59596be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn4_oh, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn4_onehop_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn4_oh_rm, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn4_rm_onehop_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f5b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn5_random, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn5_random_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39968eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn5_oh, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn5_onehop_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa27825",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "plt.hist(explanation_size_list_syn4_oh_rm, density=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel('Explanation Size', fontsize=17)\n",
    "plt.ylabel('Density', fontsize=17)\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(10, 7)\n",
    "figure.savefig('plots/explanation_size_syn5_rm_onehop_baseline.png', dpi=100)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CFGNNExplainerKernel",
   "language": "python",
   "name": "cfgnnexplainerkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
